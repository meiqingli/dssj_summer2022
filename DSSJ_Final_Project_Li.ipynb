{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSSJ_Final_Project_Li.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_wvETt4AXba-"
      ],
      "authorship_tag": "ABX9TyOxHjx+e4StLcEDhRzrdkeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meiqingli/dssj_summer2022/blob/main/DSSJ_Final_Project_Li.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Geographic Disparity in Publishing: A Sentiment Analysis from Subreddit Community**\n",
        "**Meiqing Li | PhD Candidate in City and Regional Planning**\n",
        "\n",
        "**DSSJ @ Berkeley**"
      ],
      "metadata": {
        "id": "fXV5DkoFXUDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N3axAwmjSpVV"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "# load the English preprocessing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "The subreddit of interest is [Publishing](https://www.reddit.com/r/publishing/), which is a community for publishing professionals to discuss various issues such as career and business opportunities. Through initial qualitative analysis, we found it a relatively small community compared with other subreddits by number of readers and posts. In terms of topics, we found discussions on different types of publishing, for example self-publishing,publishing through agent, and digital publishing, as well as publishing in the US vs. Canada. These are all potential areas for further investigation. \n",
        "\n",
        "Before cleaning up, our dataset includes 7,330 posts and 19,538 comments. By removing blank texts, we reduced the size of dataset to 2,388 posts and 19,054 comments. There have been a total of 1,922 unique users for posts, and 4,033 unique users for comments. Interestingly, we noticed that the most frequent user's posts take up 1% of the overall posts while the next only takes up 0.3%. Another observation we find interesting is that people are curious about the differences in publishing between US and Canada, whilst it seems that most of the posts are from people trying to navigate publishing but very little from the publisher side. For this project, I would like to look into the posts by different people involved in publishing, specifically the themes and sentiments within posts from different areas or regions. For example, how do the sentiments and concerns regarding publishing differ between US, Canada and other countries, or different parts of the US? Also, within each geographic region, what are the shares of posts by people interested in publishing, publishers, agents, and other stakeholders? I am interested in studying these dynamics because they can give us a better understanding about the broader landscape of publishing. \n",
        "\n",
        "In order to address these questions, I will look into several NLP techniques inlcuding *topic modeling* and *sentiment analysis*. Specifically, I will use tools such as *concordance* to identify texts with geographical key words, then extract topical words and sentiments. "
      ],
      "metadata": {
        "id": "_wvETt4AXba-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Github url with post data\n",
        "url_posts = 'https://gist.githubusercontent.com/meiqingli/2d128e1cac170d71b3820c51c6c3766e/raw/5be159378011713f1daaca4d683bb6875d11f61f/submissions.csv'\n",
        "\n",
        "# Github url with comments data\n",
        "url_comments = 'https://gist.githubusercontent.com/meiqingli/5d83de4c508a0564359b2dd07c6839b1/raw/a83a44b0a85ac8ccc10f717495ff6a85c2f93878/comments.csv'\n",
        "\n",
        "# Reads the csv posts file from github\n",
        "df_posts = pd.read_csv(url_posts)\n",
        "\n",
        "# Reads the csv comments file from github\n",
        "df_comments = pd.read_csv(url_comments)"
      ],
      "metadata": {
        "id": "PCbt02LoafQ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tells us about the general shape of the dfs\n",
        "df_posts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkHxETBKBCrL",
        "outputId": "49fc52d3-d29b-4e8d-8b6c-8c6d6ed16105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7330, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_comments.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXssdSUkA7QC",
        "outputId": "693427db-27e0-4fb9-c2fb-b91c8ad6a1d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19538, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Allows us to see which columns our dfs have\n",
        "list(df_posts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkRon2gdA_Ew",
        "outputId": "98614b74-38f7-4d05-ae40-880fc28db00a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['idint',\n",
              " 'idstr',\n",
              " 'created',\n",
              " 'self',\n",
              " 'nsfw',\n",
              " 'author',\n",
              " 'title',\n",
              " 'url',\n",
              " 'selftext',\n",
              " 'score',\n",
              " 'subreddit',\n",
              " 'distinguish',\n",
              " 'textlen',\n",
              " 'num_comments',\n",
              " 'flair_text',\n",
              " 'flair_css_class',\n",
              " 'augmented_at',\n",
              " 'augmented_count']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(df_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozN08eRiBNIU",
        "outputId": "d03d4501-10b2-405e-9aa0-c18a9c852b6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['idint',\n",
              " 'idstr',\n",
              " 'created',\n",
              " 'author',\n",
              " 'parent',\n",
              " 'submission',\n",
              " 'body',\n",
              " 'score',\n",
              " 'subreddit',\n",
              " 'distinguish',\n",
              " 'textlen']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "We mainly made the following preprocessing steps to the original dataset:\n",
        "\n",
        "*   Drop redundant columns;\n",
        "*   Remove rows in posts and comments that are either \"removed\" or \"deleted\";\n",
        "*   Drop rows with null values in `selftext` and `body` columns;\n",
        "*   Drop duplicate posts and columns;\n",
        "*   Text cleaning using `spaCy`;\n",
        "*   Phrase modeling with `gensim`;\n",
        "*   Save preprocessed texts to new data frames for further analysis.\n",
        "\n",
        "Code from this section is largely based on joint work by Madeline Bossi, Soliver Fusi, Janiya Peters, and me. "
      ],
      "metadata": {
        "id": "6vxqz7JoXeqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drops less useful columns\n",
        "\n",
        "df_posts_short = df_posts.drop(['subreddit', 'url', 'distinguish', 'flair_text', 'flair_css_class', 'augmented_at', 'augmented_count'], axis=1)\n",
        "df_comments_short = df_comments.drop(['subreddit', 'distinguish'], axis=1)"
      ],
      "metadata": {
        "id": "epYNsfAc6Br8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selects all rows that don't have 'removed' or 'deleted' in certain columns\n",
        "df_comments_noBlanks = df_comments_short.loc[~df_comments_short['body'].isin(['[removed]', '[deleted]' ]),:]\n",
        "df_posts_noBlanks = df_posts_short.loc[~df_posts_short['selftext'].isin(['[removed]', '[deleted]' ]),:]"
      ],
      "metadata": {
        "id": "vlX-4OxsmNe7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drops rows with null values in 'selftext' or 'body' column (assuming we want to analyze 'selftext')\n",
        "df_posts_noBlanks = df_posts_noBlanks.dropna(subset=['selftext'])\n",
        "df_comments_noBlanks = df_comments_noBlanks.dropna(subset=['body'])"
      ],
      "metadata": {
        "id": "DVn7tVgDmNhZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop duplicate posts or comments\n",
        "df_posts_noBlanks = df_posts_noBlanks.drop_duplicates(subset=['selftext'])\n",
        "df_comments_noBlanks = df_comments_noBlanks.drop_duplicates(subset=['body'])"
      ],
      "metadata": {
        "id": "JXqAgvLbmNpB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final number of posts\n",
        "df_posts_noBlanks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuKFSZzJmNkC",
        "outputId": "95140eee-81a2-49e9-bb85-fc02c4c0fa12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2385, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final number of comments\n",
        "df_comments_noBlanks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbsOsJbQmNmn",
        "outputId": "bbe12856-d7ab-40e0-a305-052ff8a200da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18754, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our original dataset has 7,330 rows of posts and 19,538 rows of comments. After preprocessing, the dataset contains 2,385 posts and 18,754 comments, which are small subsets of the original ones. "
      ],
      "metadata": {
        "id": "3WPnJ4Y7xIzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(token):\n",
        "    \"\"\"Helper function that specifies whether a token is:\n",
        "        - punctuation\n",
        "        - space\n",
        "        - digit\n",
        "    \"\"\"\n",
        "    return token.is_punct or token.is_space or token.is_digit\n",
        "\n",
        "def line_read(df, text_col='selftext'):\n",
        "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
        "    for text in df[text_col]:\n",
        "        yield text.replace('\\n', '')\n",
        "\n",
        "def preprocess_posts(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
        "    \"\"\"Preprocessing function to apply to the posts dataframe.\"\"\"\n",
        "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"ner\"]):\n",
        "        # Gather lowercased, lemmatized tokens\n",
        "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
        "                  else token.lower_ \n",
        "                  for token in parsed if not clean(token)]\n",
        "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
        "        tokens = [lemma\n",
        "                  for lemma in tokens\n",
        "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
        "        # Remove stop words\n",
        "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
        "        yield tokens\n",
        "\n",
        "# created a different preprocessing function for the comments df because the relevant column name is different\n",
        "def preprocess_comments(df, text_col='body', allowed_postags=['NOUN', 'ADJ']):\n",
        "    \"\"\"Preprocessing function to apply to the comments dataframe.\"\"\"\n",
        "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"ner\"]):\n",
        "        # Gather lowercased, lemmatized tokens\n",
        "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
        "                  else token.lower_ \n",
        "                  for token in parsed if not clean(token)]\n",
        "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
        "        tokens = [lemma\n",
        "                  for lemma in tokens\n",
        "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
        "        # Remove stop words\n",
        "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
        "        yield tokens\n"
      ],
      "metadata": {
        "id": "4OZ_GSZ8mNrS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a list of lists of lemmas in each post\n",
        "lemmas_posts = [line for line in preprocess_posts(df_posts_noBlanks)]\n",
        "\n",
        "# creates a list of lists of lemmas in each comment\n",
        "lemmas_comments = [line for line in preprocess_comments(df_comments_noBlanks)]"
      ],
      "metadata": {
        "id": "7VnXdLxhmNt7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flattens the list of lists into one big list to facilitate counting lemma frequency\n",
        "flat_lemmas_posts = [item for sublist in lemmas_posts for item in sublist]\n",
        "# flat_lemmas_posts[:30]"
      ],
      "metadata": {
        "id": "Jscyb8hzmNwR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a df to see which lemmas are the most frequent\n",
        "from collections import Counter\n",
        "posts_count = Counter(flat_lemmas_posts)\n",
        "\n",
        "lemma_freq_df_posts = pd.DataFrame.from_dict(posts_count, orient='index').reset_index()\n",
        "lemma_freq_df_posts = lemma_freq_df_posts.rename(columns={'index':'lemma', 0:'count'})\n",
        "lemma_freq_df_posts.sort_values('count', ascending=False)[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "xQZgSzSHC86M",
        "outputId": "ff3fc6f6-e964-4d27-db42-67776043f95f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          lemma  count\n",
              "201        book   3134\n",
              "187     publish   1794\n",
              "2    publishing   1448\n",
              "17         work   1393\n",
              "3          like   1283\n",
              "120        know   1130\n",
              "188        want   1060\n",
              "290   publisher   1059\n",
              "174       write    971\n",
              "77         look    836\n",
              "231       think    703\n",
              "143        good    697\n",
              "194       story    694\n",
              "295      author    654\n",
              "389        time    649\n",
              "14         find    649\n",
              "133       thank    635\n",
              "54          use    610\n",
              "584        help    589\n",
              "177        year    582\n",
              "13         need    545\n",
              "247       agent    532\n",
              "109         try    529\n",
              "424      people    517\n",
              "198        self    512\n",
              "317       start    491\n",
              "1    experience    458\n",
              "397       right    444\n",
              "62         read    438\n",
              "321         new    434"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8040a70e-7236-4484-8b63-974b44878012\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>book</td>\n",
              "      <td>3134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>publish</td>\n",
              "      <td>1794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>publishing</td>\n",
              "      <td>1448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>work</td>\n",
              "      <td>1393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>like</td>\n",
              "      <td>1283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>know</td>\n",
              "      <td>1130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>want</td>\n",
              "      <td>1060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>publisher</td>\n",
              "      <td>1059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>write</td>\n",
              "      <td>971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>look</td>\n",
              "      <td>836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>think</td>\n",
              "      <td>703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>good</td>\n",
              "      <td>697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>story</td>\n",
              "      <td>694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>author</td>\n",
              "      <td>654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>time</td>\n",
              "      <td>649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>find</td>\n",
              "      <td>649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>thank</td>\n",
              "      <td>635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>use</td>\n",
              "      <td>610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>help</td>\n",
              "      <td>589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>year</td>\n",
              "      <td>582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>need</td>\n",
              "      <td>545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>agent</td>\n",
              "      <td>532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>try</td>\n",
              "      <td>529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>people</td>\n",
              "      <td>517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>self</td>\n",
              "      <td>512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>start</td>\n",
              "      <td>491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>experience</td>\n",
              "      <td>458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>right</td>\n",
              "      <td>444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>read</td>\n",
              "      <td>438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>new</td>\n",
              "      <td>434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8040a70e-7236-4484-8b63-974b44878012')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8040a70e-7236-4484-8b63-974b44878012 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8040a70e-7236-4484-8b63-974b44878012');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replicates this same process for the comments\n",
        "flat_lemmas_comments = [item for sublist in lemmas_comments for item in sublist]\n",
        "# flat_lemmas_comments[:30]"
      ],
      "metadata": {
        "id": "mX2pPsT0C9iY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_count = Counter(flat_lemmas_comments)\n",
        "\n",
        "lemma_freq_df_comments = pd.DataFrame.from_dict(comments_count, orient='index').reset_index()\n",
        "lemma_freq_df_comments = lemma_freq_df_comments.rename(columns={'index':'lemma', 0:'count'})\n",
        "lemma_freq_df_comments.sort_values('count', ascending=False)[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "t9AyzPjJC9qd",
        "outputId": "28b54d70-e627-4316-c749-4b1a0ec6d1a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          lemma  count\n",
              "4          book  10150\n",
              "72         work   5754\n",
              "322   publisher   5155\n",
              "6       publish   4775\n",
              "25   publishing   4580\n",
              "17         good   4248\n",
              "161        like   4033\n",
              "324        want   3643\n",
              "170      author   3405\n",
              "940       agent   3217\n",
              "105        know   3213\n",
              "7         think   3066\n",
              "552        look   2957\n",
              "152      people   2750\n",
              "414       write   2642\n",
              "83         need   2513\n",
              "176       thank   2474\n",
              "331        time   2360\n",
              "196         use   2202\n",
              "10         find   2173\n",
              "421        self   2115\n",
              "316         lot   2095\n",
              "86          pay   2018\n",
              "30        thing   1896\n",
              "38          job   1864\n",
              "422        sell   1863\n",
              "192         try   1861\n",
              "202       right   1861\n",
              "40          way   1827\n",
              "349        help   1758"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d398aac-9522-4149-b231-f902ba843e8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemma</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>book</td>\n",
              "      <td>10150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>work</td>\n",
              "      <td>5754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>publisher</td>\n",
              "      <td>5155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>publish</td>\n",
              "      <td>4775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>publishing</td>\n",
              "      <td>4580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>good</td>\n",
              "      <td>4248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>like</td>\n",
              "      <td>4033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>want</td>\n",
              "      <td>3643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>author</td>\n",
              "      <td>3405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>agent</td>\n",
              "      <td>3217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>know</td>\n",
              "      <td>3213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>think</td>\n",
              "      <td>3066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>look</td>\n",
              "      <td>2957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>people</td>\n",
              "      <td>2750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>write</td>\n",
              "      <td>2642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>need</td>\n",
              "      <td>2513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>thank</td>\n",
              "      <td>2474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>time</td>\n",
              "      <td>2360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>use</td>\n",
              "      <td>2202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>find</td>\n",
              "      <td>2173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>self</td>\n",
              "      <td>2115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>lot</td>\n",
              "      <td>2095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>pay</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>thing</td>\n",
              "      <td>1896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>job</td>\n",
              "      <td>1864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>sell</td>\n",
              "      <td>1863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>try</td>\n",
              "      <td>1861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>right</td>\n",
              "      <td>1861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>way</td>\n",
              "      <td>1827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>help</td>\n",
              "      <td>1758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d398aac-9522-4149-b231-f902ba843e8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d398aac-9522-4149-b231-f902ba843e8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d398aac-9522-4149-b231-f902ba843e8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forming bigrams and trigrams\n",
        "\n",
        "# Create bigram and trigram models for posts\n",
        "bigram_posts = Phrases(lemmas_posts, min_count=10, threshold=100)\n",
        "trigram_posts = Phrases(bigram_posts[lemmas_posts], min_count=10, threshold=50)  \n",
        "bigram_phraser_posts = Phraser(bigram_posts)\n",
        "trigram_phraser_posts = Phraser(trigram_posts)\n",
        "\n",
        "# Form trigrams\n",
        "trigrams_posts = [trigram_phraser_posts[bigram_phraser_posts[doc]] for doc in lemmas_posts]\n",
        "\n",
        "# Create bigram and trigram models for comments\n",
        "bigram_comments = Phrases(lemmas_comments, min_count=10, threshold=100)\n",
        "trigram_comments = Phrases(bigram_comments[lemmas_comments], min_count=10, threshold=50)  \n",
        "bigram_phraser_comments = Phraser(bigram_comments)\n",
        "trigram_phraser_comments = Phraser(trigram_comments)\n",
        "\n",
        "# Form trigrams\n",
        "trigrams_comments = [trigram_phraser_comments[bigram_phraser_comments[doc]] for doc in lemmas_comments]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BymEPpXgC9t2",
        "outputId": "f571f7ed-fa2a-49f5-b5c3-356007e54859"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joins each into a string\n",
        "trigrams_joined_posts = [' '.join(trigram) for trigram in trigrams_posts]\n",
        "trigrams_joined_posts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vTg2jpqgC9wt",
        "outputId": "fc3298cd-684e-4254-f333-d664b5e052b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'industry experience publishing like enter field specifically editing usual catch-22 hold true job need experience experience need job find entry_level work prove difficult live baltimore md feel exhaust obvious avenue send resume cover_letter magazine newspaper publishing house etc locate oppose internship course college student tend exclude consideration additionally support work pay obvious sense priority realize reader subreddit shot dark use advice person familiar industry consider send email editor blog read ask similar guidance perceive brazen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# joins each into a string\n",
        "trigrams_joined_comments = [' '.join(trigram) for trigram in trigrams_comments]\n",
        "trigrams_joined_comments[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3PEKR6JsEYDX",
        "outputId": "54acf04c-e6ab-4043-d7af-6c8d29dd5cae"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'great tip start market book online publish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adds lemmas column to submissions df (with option to make csv)\n",
        "df_posts_noBlanks.insert(loc=7, column='lemmas', value=trigrams_joined_posts)\n",
        "df_posts_noBlanks = df_posts_noBlanks[~df_posts_noBlanks['lemmas'].isin([''])]\n",
        "df_posts_noBlanks.to_csv('submissions_lemmas.csv', index=False)\n",
        "\n",
        "# adds lemmas column to comments df (with option to make csv)\n",
        "df_comments_noBlanks.insert(loc=7, column='lemmas', value=trigrams_joined_comments)\n",
        "df_comments_noBlanks = df_comments_noBlanks[~df_comments_noBlanks['lemmas'].isin([''])]\n",
        "df_comments_noBlanks.to_csv('comments_lemmas.csv', index=False)"
      ],
      "metadata": {
        "id": "rDX3rzJ0EYLc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JZLQ8bkmEYOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SCOMFWXpEYSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y-iCo1nkEYVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jnu7qwmCEYYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Github url with post data\n",
        "url_posts_processed = ''\n",
        "\n",
        "# Github url with comments data\n",
        "url_comments_processed = ''\n",
        "\n",
        "# Reads the csv posts file from github\n",
        "df_posts_processed = pd.read_csv(url_posts_processed)\n",
        "\n",
        "# Reads the csv comments file from github\n",
        "df_comments_processed = pd.read_csv(url_comments_processed)"
      ],
      "metadata": {
        "id": "3JfQXyKIBi0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare with the original dataset\n",
        "# Identify any differences from the quantitative exploration in the Introduction (less words, fewer unique posters, etc)."
      ],
      "metadata": {
        "id": "LrMn2AFffgtH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VeOjKsuqfgb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "A9yJKmTLXhDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "This section is where you get to deploy and discuss your analysis. Address the following points:\n",
        "\n",
        "Why is this analysis appropriate for the research question?\n",
        "Include the code for performing the analysis. Note any analytical choices made (for example choosing parameters)\n",
        "Interpret the results of your analysis\n",
        "Discuss how the analysis relates to your research question\n",
        "Address each research question (if you have multiple) in a separate section, and include a subsection (###) for each technique used. Again, the goal is to weave the code and text together, the same way you would weave figures and text together in a traditional research paper. Speaking of figures- try to include at least one visualization of your results (a table, figure, etc) per quantitative analysis.\n",
        "\n",
        "The balance of quantitative and quantitative techniques will depend a bit on your project, but you should have at least some quantitative and qualitative elements for research question.\n",
        "\n",
        "A sample analysis outline is included below:\n",
        "```"
      ],
      "metadata": {
        "id": "r-X7FwbZuwTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis 1"
      ],
      "metadata": {
        "id": "xdllWWDLXiuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "A sample tf-idf analysis.\n",
        "\n",
        "Describe tf-idf;\n",
        "Describe choices made (i.e. parameters of tf-idf);\n",
        "Include predictions/original expectations for the analysis.\n",
        "```"
      ],
      "metadata": {
        "id": "GDyB2n1Lu0B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# incorporating time\n",
        "# \n",
        "datetimes = pd.to_datetime(df['created'], unit='s')"
      ],
      "metadata": {
        "id": "Dc7vuvo-PIVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "*   Comment on the graph of tf-idf scores. Does it meet expectations?\n",
        "*   Comment on the highest tf-idf scores.\n",
        "```"
      ],
      "metadata": {
        "id": "FwRE9zEIu9Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Conclude the analysis:\n",
        "\n",
        "How did this analysis address your research question?\n",
        "Reflect on any new or unexpected patterns, consider further exploratory analysis into those patterns.\n",
        "Identify any limitations of the analysis and how they may impact the results.\n",
        "And then on to the next analysis/question.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LNPAQfFdu6YX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has already been found that a very small percentage of Reddit’s users create the vast majority of the site’s content, so we would not be surprised if only a few users could influence the discourse of entire subreddits. Identifying these users would help us understand how a subreddit's discourse is shaped. "
      ],
      "metadata": {
        "id": "9LdhubQI0t9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values(by='score', ascending=False)[:1000]\n",
        "# Sanity check\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "mHnYbA9a00nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.author.nunique()"
      ],
      "metadata": {
        "id": "d2nCBOPF01zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot('score', 'num_comments', kind='scatter', color='black', alpha=0.25, logy=True)"
      ],
      "metadata": {
        "id": "AeEoMRsw03fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nyURoT2U07hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Rank by geographic areas, map distribution\n",
        "## Word embedding and topic modeling\n",
        "## Cluster Analysis\n",
        "## Sentiment analysis\n",
        "## Network analysis"
      ],
      "metadata": {
        "id": "YfKq-U-UahOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "yBQljCpEaVKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "This section is a brief summary of your analyses and final thoughts:\n",
        "\n",
        "What was your conclusions in response to your research questions?\n",
        "What are potential implications of these results to the broader community?\n",
        "Reflect on how this project relates the the themes of the workshop.\n",
        "How would you further develop this research project?\n",
        "```"
      ],
      "metadata": {
        "id": "n_1VrbWcvGxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "*   Getting Started with Sentiment Analysis using Python. (n.d.). Retrieved July 17, 2022, from https://huggingface.co/blog/sentiment-analysis-python\n",
        "*   King, G., Pan, J., & Roberts, M. E. (2013). How Censorship in China Allows Government Criticism but Silences Collective Expression. American Political Science Review, 107(2), 326–343. https://doi.org/10.1017/S0003055413000014\n",
        "*   Lucy, L., Demszky, D., Bromley, P., & Jurafsky, D. (2020). Content Analysis of Textbooks via Natural Language Processing: Findings on Gender, Race, and Ethnicity in Texas U.S. History Textbooks. AERA Open, 6(3), 233285842094031. https://doi.org/10.1177/2332858420940312\n",
        "\n"
      ],
      "metadata": {
        "id": "xU8H4uChaYfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools:**\n",
        "*   [Sentiment analysis](https://github.com/cjhutto/vaderSentiment)\n",
        "*   [Getting started with sentiment analysis](https://huggingface.co/blog/sentiment-analysis-python)"
      ],
      "metadata": {
        "id": "XEl_ijy9aasT"
      }
    }
  ]
}